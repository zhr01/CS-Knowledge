# 特征选择

特征选择又称为变量选择或属性选择。

与降维的异同点：

特征选择和降维有着些许的相似点，这两者达到的效果是一样的，就是试图去减少特征数据集中的属性(或者称为特征)的数目；但是两者所采用的方式方法却不同：降维的方法主要是通过属性间的关系，如组合不同的属性得新的属性，这样就改变了原来的特征空间；而特征选择的方法是从原始特征数据集中选择出子集，是一种包含的关系，没有更改原始的特征空间。

优点：

- 减少过拟合
- 改善精度
- 减少训练时间



特征选择的目标：

- 提高预测的准确性
- 构造更快，消耗更低的预测模型
- 能够对数据生成的潜在过程有更好的理解和解释



## 特征选择的算法：

- Filter方法
- Wrapper方法
- Embedded方法



### Filter方法

其主要思想是：对每一维的特征“打分”，即给每一维的特征赋予权重，这样的权重就代表着该维特征的重要性，然后依据权重排序。

主要的方法有：

- Chi-squared test(卡方检验)
- information gain(信息增益)——决策树算法
- correlation coefficient scores(相关系数)



### Wrapper方法

其主要思想是：将子集的选择看作是一个搜索寻优问题，生成不同的组合，对组合进行评价，再与其他的组合进行比较。这样就将子集的选择看作是一个是一个优化问题，这里有很多的优化算法可以解决，尤其是一些启发式的优化算法，如GA，PSO，DE，ABC(人工蚁群算法)等，

主要方法有：recursive feature elimination algorithm(递归特征消除算法)



### Embedded方法

其主要思想是：在模型既定的情况下学习出对提高模型准确性最好的属性。这句话并不是很好理解，其实是讲在确定模型的过程中，挑选出那些对模型的训练有重要意义的属性。最常用的的正则化（regularization ）方法。

常用的正则化算法有LASSO, Elastic Net 和 Ridge Regression（脊回归）



做特征选择的经验清单：

> 1. **Do you have domain knowledge?** If yes, construct a better set of ad hoc”” features
> 2. **Are your features commensurate?** If no, consider normalizing them.
> 3. **Do you suspect interdependence of features?** If yes, expand your feature set by constructing conjunctive features or products of features, as much as your computer resources allow you.
> 4. **Do you need to prune the input variables (e.g. for cost, speed or data understanding reasons)?** If no, construct disjunctive features or weighted sums of feature
> 5. **Do you need to assess features individually (e.g. to understand their influence on the system or because their number is so large that you need to do a first filtering)?** If yes, use a variable ranking method; else, do it anyway to get baseline results.
> 6. **Do you need a predictor?** If no, stop
> 7. **Do you suspect your data is “dirty” (has a few meaningless input patterns and/or noisy outputs or wrong class labels)?** If yes, detect the outlier examples using the top ranking variables obtained in step 5 as representation; check and/or discard them.
> 8. **Do you know what to try first?** If no, use a linear predictor. Use a forward selection method with the “probe” method as a stopping criterion or use the 0-norm embedded method for comparison, following the ranking of step 5, construct a sequence of predictors of same nature using increasing subsets of features. Can you match or improve performance with a smaller subset? If yes, try a non-linear predictor with that subset.
> 9. **Do you have new ideas, time, computational resources, and enough examples?** If yes, compare several feature selection methods, including your new idea, correlation coefficients, backward selection and embedded methods. Use linear and non-linear predictors. Select the best approach with model selection
> 10. **Do you want a stable solution (to improve performance and/or understanding)?** If yes, subsample your data and redo your analysis for several “bootstrap”.

## 代码实现

### 使用sklearn实现特征选择

#### Recursive Feature Elimination

RFE算法递归移除特征，然后在剩余的特征上建立模型，通过比较在不同特征（组合）下模型的精度确定哪些特征对预测的目标贡献最大。

```python
# Recursive Feature Elimination
from sklearn import datasets
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
# load the iris datasets
dataset = datasets.load_iris()
# create a base classifier used to evaluate a subset of attributes
model = LogisticRegression()
# create the RFE model and select 3 attributes
rfe = RFE(model, 3)
rfe = rfe.fit(dataset.data, dataset.target)
# summarize the selection of the attributes
print(rfe.support_)
print(rfe.ranking_)
```

output：

[False  True  True  True]
[2 1 1 1]



### Feature Importance

Methods that use ensembles of decision trees (like Random Forest or Extra Trees) can also compute the relative importance of each attribute. These importance values can be used to inform a feature selection process.

This recipe shows the construction of an Extra Trees ensemble of the iris flowers dataset and the display of the relative feature importance.

```python
# Feature Importance
from sklearn import datasets
from sklearn import metrics
from sklearn.ensemble import ExtraTreesClassifier
# load the iris datasets
dataset = datasets.load_iris()
# fit an Extra Trees model to the data
model = ExtraTreesClassifier()
model.fit(dataset.data, dataset.target)
# display the relative importance of each attribute
print(model.feature_importances_)
```















参考文献：

http://machinelearningmastery.com/an-introduction-to-feature-selection/

http://machinelearningmastery.com/feature-selection-in-python-with-scikit-learn/

http://machinelearningmastery.com/feature-selection-machine-learning-python/

http://blog.csdn.net/google19890102/article/details/40019271