# 梯度下降法

在求解函数模型时，可用 最小二乘法（OLS），当样本数据较少时，可直接用代数的方法求损失函数接的解析式，当样本数量较多时，矩阵的求解计算量非常大，于是在实际应用中采用计算法求损失函数的解。

**梯度下降法（Gradient Descent）**是一种常见的最优化算法，用于求解函数的最大值或者最小值。

一个函数$J(\theta)$对变量$\theta$的梯度定义为：
$$
\frac{\partial J(\theta)}{\partial \theta} = \mathop{\lim}_{\Delta \theta \to 0}\frac{J(\theta + \Delta \theta) - J(\theta)}{\Delta \theta}
$$
为什么梯度反方向是函数值下降最快的方向？

**导数**

几何意义：函数曲线上切线的斜率

物理意义：函数在该点的瞬时变化率，如速度

**偏导数**

以两个自变量为例，z=f(x,y) . 从导数到偏导数，也就是从曲线来到了曲面. 曲线上的一点，其切线只有一条。但是曲面的一点，切线有无数条。而我们所说的偏导数就是指的是多元函数沿坐标轴的变化率.

$f_x(x,y)$指的是函数在y方向不变，函数值沿着x轴方向的变化率

$f_y(x,y)$指的是函数在x方向不变，函数值沿着y轴方向的变化率

![](pic/partial.png)

偏导数对应的几何意义：

- 偏导数$f_x(x,y)$就是曲面被平面所$y=y_0$截得的曲面在点$M_0$处的切线$M_0T_x$对x轴的斜率
- 偏导数$f_y(x,y)$就是曲面被平面所$x=x_0$截得的曲面在点$M_0$处的切线$M_0Y_y$对y轴的斜率

偏导数指的是多元函数沿坐标轴的变化率，但是我们往往很多时候要考虑多元函数沿任意方向的变化率，那么就引出了方向导数.

**方向导数**

对于函数z=f(x,y)，若要求其在$u=(cos\theta, sin\theta)$方向上的导数，可以类比前面导数的定义，令：
$$
x= x_0 + tcos\theta \\
y = y_0 + tsin\theta
$$
则其导数定义为：
$$
D_uf = \mathop{\lim}_{t \to 0}\frac{f(x_0 + tcos\theta, y_0+tsin\theta)-f(x_0,y_0)}{t}
$$
则称这个极限值是f沿着u方向的方向导数，那么随着$\theta$的不同，我们可以求出任意方向的方向导数.这也表明了方向导数的用处，是为了给我们考虑函数对任意方向的变化率.

定理：若函数$f$在点$p_0$处可微，向量l的方向余弦为$cos\alpha_1,cos\alpha_2,\cdot\cdot\cdot,cos\alpha_n$，则函数f在点$p_0$处沿u方向的方向导数存在，且
$$
D_uf(x_1,x_2,...,x_n)=f_{x_1}^{'}cos\alpha_1 +f_{x_2}^{'}cos\alpha_2+...+f_{x_n}^{'}cos\alpha_n
$$
设$A=(f_x(x,y),f_y(x,y)),\ I=(cos\theta,sin\theta)$，对于方向导数：
$$
D_uf(x,y)=f_x(x,y)cos\theta + f_u(x,y)sin\theta \\
= A\cdot I=|A|\times |I|cos\alpha
$$
$\alpha$为向量A与向量I之间的夹角，当$\alpha=0$时，也就是向量I（这个方向是一直在变，在寻找一个函数变化最快的方向）与向量A（这个方向当点固定下来的时候，它就是固定的）平行的时候，方向导数最大.方向导数最大，也就是单位步伐，函数值朝这个反向变化最快.

在神经网络模型中，模型的损耗函数$J(w,b)$是关于$w,b$的函数，要使函数$J(w,b)$的值减小，按照上述推论，就得是$w,b$朝梯度反方向移动，即：
$$
w \to w-\alpha \nabla J_w(w,b) \\
b \to b-\alpha \nabla J_b(w,b)  \tag{1}
$$
其中，$\alpha$为学习速率（为正值）。



**损耗函数的定义**

设拟合函数为：
$$
h(\theta) = \theta_0 + \theta_1x_1+...+\theta_nx_n = \sum_{j=1}^m\theta_jx_j = \theta^TX \tag{2}
$$
则整体损耗函数的实际值与测量值平方差的均值：
$$
J(\theta)=\frac{1}{2m}\sum_{i=1}^m(h_{\theta}(x_i)-y_i)^2 \tag{3}
$$
梯度下降的目标就是选取适当的参数$\theta$，最小化$J(\theta)$函数



### 批量梯度下降（Batch Gradient Descent,BGD）

Use all examples in each iteration；

**优点：**全局最优解；易于并行实现；

**缺点：**当样本数目很多时，训练过程会很慢。

对每个$\theta_j$求偏导，得到每个$\theta_j$对应的梯度：
$$
\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m}\sum_{i=1}^m(h_{\theta(x^i)-y^i})x_j^i  \tag{4}
$$
其中，$x_j^i$表示第i个样本点$x^i$的第j个分量。根据（1）式得到各个参数的$\theta_j$的更新值：
$$
\theta_j = \theta_j - \alpha \frac{\partial J(\theta)}{\partial \theta_j}  = 
\theta_j -  \frac{\alpha}{m}\sum_{i=1}^m(h_{\theta(x^i)-y^i})x_j^i \tag{5}
$$
从上面公式可以注意到，它得到的是一个全局最优解，但是每迭代一步，都要用到训练集所有的数据，如果m很大，那么可想而知这种方法的迭代速度！！所以，这就引入了随机梯度下降。

### 随机梯度下降法（Stochastic Gradient Descent, SGD）

Use 1 example in each iteration；

**优点：**训练速度快；

**缺点：**准确度下降，并不是全局最优；不易于并行实现。

### 小批量梯度下降法（Mini-batch Gradient Descent, MBGD）

Use b examples in each iteration.

### Python代码实现

BGD:

```python
from numpy import *

# y = mx + b
# m is slope, b is y-intercept
def compute_error_for_line_given_points(b, m, points):
    totalError = 0
    for i in range(0, len(points)):
        x = points[i, 0]
        y = points[i, 1]
        totalError += (y - (m * x + b)) ** 2
    return totalError / float(len(points))

def step_gradient(b_current, m_current, points, learningRate):
    b_gradient = 0
    m_gradient = 0
    N = float(len(points))
    for i in range(0, len(points)):
        x = points[i, 0]
        y = points[i, 1]
        b_gradient += -(2/N) * (y - ((m_current * x) + b_current))
        m_gradient += -(2/N) * x * (y - ((m_current * x) + b_current))
    new_b = b_current - (learningRate * b_gradient)
    new_m = m_current - (learningRate * m_gradient)
    return [new_b, new_m]

def gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iterations):
    b = starting_b
    m = starting_m
    for i in range(num_iterations):
        b, m = step_gradient(b, m, array(points), learning_rate)
    return [b, m]

def run():
    points = genfromtxt("data.csv", delimiter=",")
    learning_rate = 0.0001
    initial_b = 0 # initial y-intercept guess
    initial_m = 0 # initial slope guess
    num_iterations = 1000
    print('Starting gradient descent at b = {0}, m = {1}, error = {2}'.format(initial_b, initial_m, compute_error_for_line_given_points(initial_b, initial_m, points)))
    print("Running...")
    [b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_iterations)
    print("After {0} iterations b = {1}, m = {2}, error = {3}".format(num_iterations, b, m, compute_error_for_line_given_points(b, m, points)))

if __name__ == '__main__':
    run()

```

![](pic/gradient_descent_example.gif)

