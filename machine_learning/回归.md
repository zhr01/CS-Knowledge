# 回归

- 线性回归
  - 定义
  - 最小二乘法的本质
- Logistics回归
- 工具
  - 梯度下降算法
  - 极大似然估计

### 线性回归

定义式：

$y=ax+b$

$h_{\theta}(x)=\theta_0 + \theta_1x_1+\theta_2x_2$

$h_{\theta}(x)=\sum_{i=1}^n\theta_ix_i=\theta^TX$

其中，X与θ都是列向量。

 给定一系列样本与观测值，现在来拟合参数θ，那么什么样的参数才算是好的呢？总该有一个判断标准吧，因此，因此有了**损失函数**： 
$$
J(\theta)=\frac{1}{2}\sum_{i=1}^m(h_{\theta}(x_i)-y_i)^2
$$
上式为建立最小二乘的目标公式，$h_{\theta}(x_i)$表示样本$x_i$的理论值，$y_i$表示观测值。1/2只是为了后面计算方便添加，不会对整体产生影响。

### 用极大似然估计来解释最小二乘

我们假设观测值与理论值是有误差的，那么我们可以定义如下公式，其中$\epsilon_i$表示样本$x_i$的误差。
$$
y_i = \theta^T* x_i +\epsilon_i \tag{1}
$$
中心极限定理指出，大量相互独立的随机变量，其均值的分布以正态分布为极限。中心极限定理指出了大量随机变量之和近似服从正态分布的条件。实际问题中，很多随机现象可以看做众多因素的独立影响的综合反应，往往近似服从正态分布。这里假设误差是符合高斯分布的，且期望为0。误差既然符合高斯分布，那么我们可写出它的概率公式： 
$$
p(\epsilon_i)=\frac{1}{\sqrt{2\pi \sigma}}\exp^{-\frac{\epsilon_i^2}{2\sigma^2}} \tag{2}
$$
由（1）式得$\epsilon_i = y_i-\theta^Tx_i$，所以：
$$
p(y_i) = p( \theta^T* x_i +\epsilon_i ) =\frac{1}{\sqrt{2\pi \sigma}}\exp^{-\frac{(y_i-\theta^Tx_i)^2}{2\sigma^2}}
$$
现在总共有m个样本，那么根据极大似然估计，列出似然函数
$$
L(\theta) = \prod_{i=1}^m p(y_i)= \prod_{i-1}^m \frac{1}{\sqrt{2\pi \sigma}}\exp^{-\frac{(y_i-\theta^Tx_i)^2}{2\sigma^2}}
$$
对似然函数求对数，得: 
$$
\log L(\theta) = \sum_{i=1}^m \frac{1}{\sqrt{2\pi \sigma}}\exp^{-\frac{(y_i-\theta^Tx_i)^2}{2\sigma^2}}
=m\log \frac{1}{\sqrt{2\pi \sigma}}-\frac{1}{\sigma^2}*\frac{1}{2}\sum_{i=1}^m(y_i-\theta^Tx_i)^2
$$
想要让似然函数取得最大值，则损失函数必须要取得最小值，最小二乘法得到解释。

> 最大似然估计，就是**利用已知的样本结果**，**反推最有可能（最大概率）导致这样结果的参数值。**



### Logistics回归

Logistics回归虽然名字叫回归，但其实际上是一种分类方法。

