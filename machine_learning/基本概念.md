# 基本概念

统计问题的解决步骤：

1. 数据 -收集 使用可靠来源的数据
2. 描述性统计 - 计算能总结数据的统计量，并评测各种数据可视化的方法
3. 探索性数据分析 - 寻找模式、差异和其他能解答我们问题的特征。同时，我们会检查不一致性，并确认其局限性
4. 假设检验 - 在发现明显的影响时，我们需要评判这种影响是否真实，也就是说是否是因为随机因素造成的
5. 估计 - 我们会用样本数据推断全部人口的特征



描述性统计量：均值/平均值、方差、分布、直方图、概率质量函数、条件概率

- 平均值没办法了解数据之间的差异，于是有了方差
- 方差只能描述数据之间的差异，但无法描述数据的分布，于是有了分布和直方图
- 直方图没有办法套入数学公式体系进行快速推导，于是有了概率质量函数
- 概率质量函数没办法处理附加条件的情况，于是有了条件概率

### 均方差（Mean Square Error，MSE）

各测量值误差的平方和的平均值的平方根。

设n个测量值的误差为ε1、ε2……εn，则这组测量值的标准误差σ等于：
$$
\sigma = \sqrt{\frac{\epsilon_1^2 + \epsilon_2^2 + ... + \epsilon_n^2}{n}} = \sqrt{\frac{\sum{\epsilon_i^2}}{n}}
$$
数理统计中均方误差是指参数估计值与参数真值之差平方的期望值

### 条件概率、联合概率、边缘概率

假设变量M的取值为A、B、C，以及N的取值为X、Y、Z。 
有几个明显的结论：

```
P(X|A)+P(X|B)+P(X|C)不一定等于1
P(X,A)+P(X,B)+P(X,C)=P(X)
```

假设P(X|A)=P(X|B)=P(X|C)，也不表示P(X,A)=P(X,B)=P(X,C)成立

独立性：

假设

```
P(X|A)=P(X|B)=P(X|C)
P(Y|A)=P(Y|B)=P(Y|C)
P(Z|A)=P(Z|B)=P(Z|C)
```

则可讨论一致性

```
P(X|A)=P(X|B)=P(X|C)=P(A)
P(Y|A)=P(Y|B)=P(Y|C)=P(B)
P(Z|A)=P(Z|B)=P(Z|C)=P(C)
```

即对于每一种M，其XYZ的条件概率都相等，这是一致性 
反过来，对于每一种N，其ABC的条件概率 也相等，这也是一致性

故，称M与N是独立的。

联合概率指类似于P(X=a,Y=b)这样，包含多个条件，且所有条件**同时**成立的概率

边缘概率指类似于P(X=a），P(Y=b)这样，仅与单个随机变量有关的概率

联合分布可求边缘分布，但若只知道边缘分布，无法求得联合分布。



### 最小二乘法（Least Square Methods， LSM）

我们以最简单的一元线性模型来解释最小二乘法。什么是一元线性模型呢？ 监督学习中，如果预测的变量是离散的，我们称其为分类（如决策树，支持向量机等），如果预测的变量是连续的，我们称其为回归。回归分析中，如果只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析。对于二维空间线性是一条直线；对于三维空间线性是一个平面，对于多维空间线性是一个超平面...

 对于一元线性回归模型, 假设从总体中获取了n组观察值（X1，Y1），（X2，Y2）， …，（Xn，Yn）。对于平面中的这n个点，可以使用无数条曲线来拟合。要求样本回归函数尽可能好地拟合这组值。综合起来看，这条直线处于样本数据的中心位置最合理。 选择最佳拟合曲线的标准可以确定为：使总的拟合误差（即总残差）达到最小。有以下三个标准可以选择：

1. 用“残差和最小”确定直线位置是一个途径。但很快发现计算“残差和”存在相互抵消的问题。
2. 用“残差绝对值和最小”确定直线位置也是一个途径。但绝对值的计算比较麻烦。
3. 最小二乘法的原则是以“残差平方和最小”确定直线位置。

最常用的是普通最小二乘法（ Ordinary  Least Square，OLS）：所选择的回归模型应该使所有观察值的残差平方和达到最小。（Q为残差平方和）- 即采用平方损失函数。

样本回归模型：
$$
Y_i = \hat{\beta}_0 + \hat{\beta}_1 X_i + e_i\\
\Longrightarrow \ e_i = Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i
$$
其中$e_i$为样本$(X_i,Y_i)$的误差。

平方损失函数：
$$
Q= \sum_{i=1}^n e_i^2 = \sum_{i=1}^n(Y_i- \hat{\beta}_0 - \hat{\beta}_1 X_i)^2
$$
则通过Q最小确定这条直线，即确定$\hat{\beta}_0 ,\hat{\beta}_1$，以$\hat{\beta}_0 ,\hat{\beta}_1$为变量，把它们看作是Q的函数，就变成了一个求极值的问题，可以通过求导数得到。求Q对两个待估参数的偏导数：
$$
\cases{
  \frac{\partial Q}{\partial \hat{\beta}_0}=2\sum_{i=1}^{n} (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i)(-1)=0 \\
   \frac{\partial Q}{\partial \hat{\beta}_1}=2\sum_{i=1}^{n} (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i)(-X_i)=0
}
$$
 根据数学知识我们知道，函数的极值点为偏导为0的点。

  解得：
$$
\hat{\beta}_1 =  \frac{n\sum X_iY_i-\sum X_i \sum Y_i}{n\sum X_i^2 - (\sum X_i)^2} \\
\hat{\beta}_0 =  \frac{\sum X_i^2 \sum Y_i- \sum X_i\sum X_iY_i}{n\sum X_i^2 - (\sum X_i)^2}
$$


### 二次规划

二次规划是非线性规划中的一类特殊数学规划问题。其一般形式可以表示为：
$$
min_x \ \ q(x)=\frac{1}{2}x^TGx + x^Tc \\
s.t. \ \ a_i^Tx \geqslant b_i
$$
其中G是[*Hessian*](http://baike.baidu.com/view/2255290.htm)矩阵，τ是有限指标集，c，x和{ai}，都是R中的向量。如果*Hessian*矩阵是半正定的，则我们说上式是一个凸二次规划，在这种情况下该问题的困难程度类似于线性规划。如果有至少一个向量满足约束并且在[可行域](http://baike.baidu.com/view/1780876.htm)有下界，则凸二次规划问题就有一个全局最小值。如果是正定的，则这类二次规划为严格的凸二次规划，那么全局最小值就是唯一的。如果是一个[不定矩阵](http://baike.baidu.com/view/4806263.htm)，则为非凸二次规划，这类二次规划更有挑战性，因为它们有多个平稳点和局部极小值点。

到目前为止，已经出现了很多求解二次规划问题的算法，如Lemke方法、内点法、有效集法、椭球算法等等



### 贝叶斯定理

$$
P(AB) = P(B|A)P(A) = P(A|B)P(B) \Rightarrow P(A|B) = \frac{P(B|A)P(A)}{P(B)}
$$

**已知：有N个苹果，和M个梨子，苹果为黄色的概率为20%，梨子为黄色的概率为80%，问，假如我在这堆水果中观察到了一个黄色的水果，问这个水果是梨子的概率是多少。**

用数学的语言来表达，就是已知P(apple) = N / (N + M), P(pear) = M / (N + M), P(yellow|apple) = 20%, P(yellow|pear) = 80%, 求P(pear|yellow).

要想得到这个答案，我们需要 1. 要求出全部水果中为黄色的水果数目。 2. 求出黄色的梨子数目

对于1) 我们可以得到 P(yellow) * (N + M), P(yellow) = p(apple) * P(yellow|apple) + P(pear) * p(yellow|pear)

对于2) 我们可以得到 P(yellow|pear) * M

 2) / 1) 可得：P(pear|yellow) = P(yellow|pear) * p(pear) / [P(apple) * P(yellow|apple) + P(pear) * P(yellow|pear)]

化简可得：P(pear|yellow) = P(yellow,pear) / P(yellow), 用简单的话来表示就是在已知是黄色的，能推出是梨子的概率P(pear|yellow)是黄色的梨子占全部水果的概率P(yellow,pear)除上水果颜色是黄色的概率P(yellow). 这个公式很简单吧。

我们将梨子代换为A，黄色代换为B公式可以写成：P(A|B) = P(A,B) / P(B), 可得：P(A,B) = P(A|B) * P(B).贝叶斯公式就这样推出来了。

#### 贝叶斯公式

机器学习问题中有一大类是分类问题，就是在给定观测数据D的情况下，求出其属于类别（也可以称为是假设h，h ∈ {h0, h1, h2…})的概率是多少, 也就是求出P(h|D), 可得：

P(h,D) = P(h|D) * P(D) = P(D|h) * P(h)

所以：P(h|D) = P(D|h) * P(h) / P(D), 对于一个数据集下面的所有数据，P(D)，恒定不变。所以可以认为P(D)为常数， 得到：P(h|D) ∝ P(D|h) * P(h)。我们往往不用知道P(h|D)的具体的值，而是知道例如P(h1|D)，P(h2|D)值的大小关系就是了。这个公式就是机器学习中的贝叶斯公 式，一般来说我们称P(h|D)为模型的后验概率，就是从数据来得到假设的概率，P(h)称为先验概率，就是假设空间里面的概率，P(D|h)是模型的 likelihood概率。

Likelihood（似然）这个概率比较容易让人迷惑，可以认为是已知假设的情况下，求出从假设推出数据的概率，在实际的机器学习过程中，往往加入了很多的假设。

#### 先验分布估计，likelihood函数选择

