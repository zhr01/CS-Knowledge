| file                           | Description                              |      |
| :----------------------------- | :--------------------------------------- | ---- |
| addition_rnn                   | An implementation of **sequence to sequence learning** for performing addition |      |
| antirectifier                  | The example demonstrates how to write **custom layers** for Keras |      |
| babi_memnn                     | Trains a **memory network** on the bAbI dataset |      |
| babi_rnn                       | Trains two **recurrent neural networks** based upon a story and a question |      |
| cifar10_cnn                    | Train a simple deep **CNN** on the CIFAR10 small images dataset |      |
| conv_filter_visualization      | **Visualization** of the filters of VGG16, via gradient ascet in input space |      |
| conv_lstm                      | This script demonstrates the use of a convolutional **LSTM** network |      |
| deep_dream                     | Deep Dreaming in Keras                   |      |
| image_ocr                      | This example uses a convolutional stack followed by a **recurrent stack** and a CTC logloss function to perform optical character recognition of generated text images |      |
| imdb_bidrectional_lstm         | Train a **Bidirectional LSTM** on the IMDB sentiment classification task |      |
| imdb_cnn                       | This example demonstrates the use of **Convolution1D** for text classification |      |
| imdb_cnn_lstm                  | Train a **recurrent convolutional network** on the IMDB sentiment classification task |      |
| imdb_fasttext                  | This example demonstrates the use of **fasttext** for text classification |      |
| imdb_lstm                      | Trains a **LSTM** on the IMDB sentiment classification task |      |
| lstm_benchmark                 | Compare **LSTM** implementations on the IMDB sentiment classification task |      |
| lstm_text_generation           | Example script to generate text from Nietzsche's writings. |      |
| mnist_acgen                    | Train an Auxiliary Classifier Generative Adversarial Network (ACGAN) on the MNIST dataset |      |
| mnist_cnn                      | Trains a **simple convnet** on the MNIST dataset | done |
| mnist_hierarchical_rnn         | This is an example of using **Hierarchical RNN** (HRNN) to classify MNIST digits |      |
| mnist_irnn                     | This is a reproduction of the **IRNN** experiment with pixel-by-pixel sequential MNIST |      |
| mnist_mlp                      | Trains a **simple deep NN** on the MNIST dataset | done |
| mnist_net2net                  | This is an implementation of **Net2Net** experiment with MNIST |      |
| mnist_siamese_graph            | Train a Siamese MLP on pairs of digits from the MNIST dataset |      |
| mnist_sklearn                  | Example of how to use sklearn wrapper    |      |
| mnist_swwae                    | Trains a stacked what-where autoencoder built on residual blocks on the MNIST dataset |      |
| mnist_transfer_cnn             | **Transfer learning** toy example        |      |
| neural_doodle                  | Neural doodle with Keras                 |      |
| neural_style_transfer          | Neural style transfer with Keras         |      |
| pretrained_word_embeddings     | This script loads **pre-trained word embeddings** into a  frozen keras Embedding layer, and uses it to train a text classifacation model on the 20 Newsgroup dataset |      |
| reuters_mlp                    | Trains and evaluate a **simple MLP** o the Reuters newswire topic calssification task |      |
| stateful_lstm                  | Example script showing how to use **stateful RNNs** to model long sequences efficiently |      |
| variational_autoencoder        | This script demonstrates how to build a variational **autoencoder** with Keras |      |
| variational_autoencoder_deconv | This script demonstrates how to build a variational **autoencoder** with keras and **deconvolution** layers |      |